# write a simple English tokenizer in Python (with list of sentences as input)
import re
def tokenizer(lis_of_sent):
    text=' '.join(lis_of_sent) #first turn the list to a string of text
    remove_punct=re.sub(r'[\+\,\.\#\@\!\?\'\"\-\_\%\$\/\(\)\{\}\[\]\&\:]+', '', text.lower()) #remove punctuations
    split_text=re.sub(r'\W', ',', remove_punct) #split the text at white space
    return split_text

lis=['but here are some of them that I can remember:', '-Tell me about a time when you gave a simple solution to a complex problem.', 
'- Tell me about a time when you had to work with limited time or resources.', 
'-Tell me about a time you had too much on your plate to deal with and how you handled getting everything done.', 
'-Tell me about a time when you couldn''t meet your own expectations on a project.', 
'There was only one coding question, which is writing a simple English tokenizer in Python.']

print(tokenizer(lis))   

#output
but,here,are,some,of,them,that,i,can,remember,tell,me,about,a,time,when,you,gave,a,simple,solution,to,a,complex,problem,,tell,me,about,a,
time,when,you,had,to,work,with,limited,time,or,resources,tell,me,about,a,time,you,had,too,much,on,your,plate,to,deal,with,and,how,you,handled,
getting,everything,done,tell,me,about,a,time,when,you,couldnt,meet,your,own,expectations,on,a,project,there,was,only,one,coding,question,which,
is,writing,a,simple,english,tokenizer,in,python

#An updated English tokenizer that would handle word internal periods and commas

import re

def tokenizer(text):
    #handle commas
    #pattern=r'\d+\,\d+'
    text=re.sub(r'\. ', ' ', text) #removing only those periods that are followed by a white space
    text=re.sub(r'\.$', ' ', text) #removing periods at sentence boundary
    #text=re.sub(r'(?<=[0-9])\,(?=[0-9])', '\,', text) ##
    text=re.sub(r'\, ', ' ', text) #removing those commas followed by a white space
    text=re.sub(r'[\!\?\-\:]+', '', text) #removing other punctuations
    my_text=text.split() #splitting the text at white space
    return my_text
text='I have a Ph.D., but my husband doesn''t have a Ph.D. Both me and my husband work at google.com. My husband earns $4,500 a month.'
print(tokenizer(text))

#output
['I', 'have', 'a', 'Ph.D.', 'but', 'my', 'husband', 'doesnt', 'have', 'a', 'Ph.D', 'Both', 'me', 'and', 'my', 'husband', 'work', 'at', 'google.com', 'My', 'husband', 'earns', '$4,500', 'a', 'month']
