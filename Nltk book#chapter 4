
#Exercise15
def my_stg(sen):
    sen1=sen.split() #slpitting the input string and making it a list
    
    fdist=nltk.FreqDist(w.lower() for w in sen1) #make a frequency distribution of the lower case list
    for key in sorted (fdist.keys()): #for every key in the sorted dictionary keys
        print (key, fdist[key]) #print the keys and their frequency distributions
        
sen='I love you and love her and love them'
my_stg(sen) 
#exercise 14
#nltk14 (did not work)
def novel10(text): 
    text1=text.read() #read the input text
    text1=text1.strip().lower().split() #remove white space, make lower case, and turn into a list and assign to a variable
    splitIndex=len(text1)/10 #divide the length by 10
    for w in text1[-splitIndex:]: #for word in last 10% of text1
        if w not in text1[:-splitIndex]: #if word not in the remainder of text1
            print[w]#print word
text=open('F:\OneDriveGSU\OneDrive - Georgia State University\Python_practice\hafsa.txt', 'r')
novel10(text)

#chapter4#exercise17
def shorten(text, n):
    text1=text.read() #reading the text file
    text1=text1.strip().lower().split() #turning the text into a lower case list
    fdist=FreqDist(text1) #creating a distionary of freq. distribution
    pro_text1= [] #creating an empty list
    ab=fdist.most_common(n) #creating a variable with most common n-words 
    pro_text1.append(ab) #appending the variable of most common n-words to the empty list
    for w in text1: #for every word in the list
        if w in pro_text1: #if that word is in the pro_text1 
            text1.remove(w) #remove that word from the list
    text2=' '.join(text1) #converting the list to a string
    return text2 #return the string
text=open('F:\OneDriveGSU\OneDrive - Georgia State University\Python_practice\Julian.txt', 'r') 
shorten(text, 10)  

#chapter4#exercise21
def my_tex(text, vocabulary):
    return(set(text).difference(set(vocabulary)))

text=['i', 'love', 'you', 'but', 'i', 'donot', 'know']
vocabulary=['i', 'love']
my_tex(text, vocabulary)

