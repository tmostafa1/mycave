
#Exercise15
def my_stg(sen):
    sen1=sen.split() #slpitting the input string and making it a list
    
    fdist=nltk.FreqDist(w.lower() for w in sen1) #make a frequency distribution of the lower case list
    for key in sorted (fdist.keys()): #for every key in the sorted dictionary keys
        print (key, fdist[key]) #print the keys and their frequency distributions
        
sen='I love you and love her and love them'
my_stg(sen) 
#exercise 14
#nltk14 (did not work)
def novel10(text): 
    text1=text.read() #read the input text
    text1=text1.strip().lower().split() #remove white space, make lower case, and turn into a list and assign to a variable
    splitIndex=len(text1)/10 #divide the length by 10
    for w in text1[-splitIndex:]: #for word in last 10% of text1
        if w not in text1[:-splitIndex]: #if word not in the remainder of text1
            print[w]#print word
text=open('F:\OneDriveGSU\OneDrive - Georgia State University\Python_practice\hafsa.txt', 'r')
novel10(text)

#chapter4#exercise17
def shorten(text, n):
    text1=text.read() #reading the text file
    text1=text1.strip().lower().split() #turning the text into a lower case list
    fdist=FreqDist(text1) #creating a distionary of freq. distribution
    pro_text1= [] #creating an empty list
    ab=fdist.most_common(n) #creating a variable with most common n-words 
    pro_text1.append(ab) #appending the variable of most common n-words to the empty list
    for w in text1: #for every word in the list
        if w in pro_text1: #if that word is in the pro_text1 
            text1.remove(w) #remove that word from the list
    text2=' '.join(text1) #converting the list to a string
    return text2 #return the string
text=open('F:\OneDriveGSU\OneDrive - Georgia State University\Python_practice\Julian.txt', 'r') 
shorten(text, 10)  

#chapter4#exercise21
def my_tex(text, vocabulary):
    return(set(text).difference(set(vocabulary)))

text=['i', 'love', 'you', 'but', 'i', 'donot', 'know']
vocabulary=['i', 'love']
my_tex(text, vocabulary)


##From nltk book_chapter#4#Exercise 31#part 1
import textwrap
def process(my_text):
    wrapper= textwrap.TextWrapper(width=50) #specify the max width of each line and assign that to a variable
    processed_text=wrapper.wrap(text=my_text) #use the wrap method to break the input text(which is an argument in the wrap method) into lines of specified max width
    print(processed_text)
##From nltk book_chapter#4#Exercise 31#part 2
def process(my_text):
    wrapper= textwrap.TextWrapper(width=50) #specify the max width of each line and assign that to a variable
    proc_text=wrapper.wrap(text=my_text) #use the wrap method to break the input text(which is an argument in the wrap method) into lines of specified max width
    proc_text1=' '.join(proc_text) 
    for word in proc_text1: #for every word in the processed text
        string_length=(len(word)-1)+10 #specify the required space after each word(except the last one) and assign it to a variable
        proc_text2=proc_text1.rjust(string_length) #call the rjust method (with the specified length as the argument) on the modified text and assign it to a new variable
    print(proc_text2) #print the processed text

my_text= 'I think live in an area where many people come from other countries is better. My reasons could be because you can have more friends from other countries that means even- even though it is Americans or just people come from other parts of the world. I think could be really productive. Actually you have the possibility to um, visit their countries as well. Sometimes if you get really good relationship with them, going to let you stay in your house, in in their house or over there in their countries. Okay, what else? I think learn more language. So when it is a community- when Taste new food, or new drinks, new tastes, new flavors, things like that'       
process(my_text)


